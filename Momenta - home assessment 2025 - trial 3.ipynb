{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c939d965-97b2-4861-9f35-fb7a9a5ded3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, TrainingArguments, Trainer\n",
    "import requests\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Download & Extract the Dataset\n",
    "DATASET_URL = \"https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/heads/master.zip\"\n",
    "DATASET_PATH = \"fsdd\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(DATASET_URL)\n",
    "    with open(\"fsdd.zip\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    with zipfile.ZipFile(\"fsdd.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(DATASET_PATH)\n",
    "\n",
    "    print(\"Dataset extracted successfully.\")\n",
    "\n",
    "# Step 2: Load Dataset & Process Audio\n",
    "audio_folder = os.path.join(DATASET_PATH, \"free-spoken-digit-dataset-master\", \"recordings\")\n",
    "audio_files = [f for f in os.listdir(audio_folder) if f.endswith(\".wav\")]\n",
    "\n",
    "labels = []\n",
    "waveforms = []\n",
    "sampling_rate = 16000  # Standard for Wav2Vec2\n",
    "\n",
    "for file in audio_files:\n",
    "    file_path = os.path.join(audio_folder, file)\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sampling_rate)(waveform)  # Resample if needed\n",
    "\n",
    "    label = file.split(\"_\")[0]  # Extract digit from filename\n",
    "    labels.append(label)\n",
    "    waveforms.append(waveform.squeeze(0))  # Remove channel dimension\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(waveforms, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load Wav2Vec2 Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# Step 3: Create Custom Dataset for Wav2Vec2\n",
    "class FSDDDataset(Dataset):\n",
    "    def __init__(self, waveforms, labels, processor):\n",
    "        self.waveforms = waveforms\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.waveforms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.processor(\n",
    "            self.waveforms[idx].numpy(), \n",
    "            sampling_rate=sampling_rate, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\",  # Ensures uniform input size\n",
    "            max_length=16000  # Adjust according to dataset\n",
    "        )\n",
    "        inputs[\"input_values\"] = inputs[\"input_values\"].squeeze(0)  # Remove batch dim\n",
    "        inputs[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return inputs  # ✅ Ensure this is inside the function\n",
    "\n",
    "\n",
    "train_dataset = FSDDDataset(X_train, y_train, processor)\n",
    "test_dataset = FSDDDataset(X_test, y_test, processor)\n",
    "\n",
    "# Step 4: Load Pretrained Wav2Vec2 Model for Classification\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=num_classes)\n",
    "\n",
    "# Step 5: Fine-Tuning with Trainer API\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Custom collate function to handle variable-length input tensors\n",
    "def collate_fn(batch):\n",
    "    input_values = [item[\"input_values\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    \n",
    "    # Pad input values to make them the same length\n",
    "    input_values_padded = pad_sequence(input_values, batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    # Convert labels to tensor\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return {\"input_values\": input_values_padded, \"labels\": labels_tensor}\n",
    "\n",
    "# Use this function in Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=processor,\n",
    "    data_collator=collate_fn  # ✅ Now it's defined\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Test Accuracy: {100 * results['eval_loss']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88113477-3260-49e5-8ada-d1a966337244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
